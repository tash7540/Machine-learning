{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 - Naive Bayes, Decision Trees with ensemble methods\n",
    "## CSCI 4622 - Fall 2021\n",
    "***\n",
    "**Name**: $Tamer Shahwan$ \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **1:50PM on October 6th**.\n",
    "\n",
    "Submit only this Jupyter notebook to Canvas. Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted.\n",
    "The only exception to this rule is that you may copy code directly from your own solution to homework 1.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "Your task for this homework is to build a naive Bayes and a decision tree classifiers in the first 2 problems.\n",
    "The last problem is about ensemble methods using scikit-learn decision tree as a weak learner.\n",
    "We'll explore bagging, boosting (AdaBoost) and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import math\n",
    "%matplotlib inline \n",
    "#additional imports to play with dictionaries\n",
    "from collections import Counter\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Naive Bayes [25 points]\n",
    "***\n",
    "Consider the problem of predicting whether a person has a college degree based on age, salary, and Colorado residency.\n",
    "The dataset looks like the following.\n",
    "\n",
    "|Age|Salary|Colorado Residency| College degree|\n",
    "|:------:|:-----------:| :----------:|--:|\n",
    "| 27 | 41,000 | Yes | Yes |\n",
    "| 61 | 52,000 | No | No |\n",
    "| 23 | 24,000 | Yes | No |\n",
    "| 29 | 77,000 | Yes | Yes |\n",
    "| 32 | 48,000 | No | Yes |\n",
    "| 57 | 120,000 | Yes | Yes |\n",
    "| 22 | 38,000 | Yes | Yes |\n",
    "| 41 | 45,000 | Yes | No |\n",
    "| 53 | 26,000 | No | No |\n",
    "| 48 | 65,000 | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = np.array([[27 , 41000 , 1],\n",
    "              [61 , 52000 , 0],\n",
    "              [23 , 24000 , 1],\n",
    "              [29 , 77000 , 1],\n",
    "              [32 , 48000 , 0],\n",
    "              [57 , 120000 , 1],\n",
    "              [22 , 38000 , 1],\n",
    "              [41 , 45000 , 1],\n",
    "              [53 , 26000 , 0],\n",
    "              [48 , 65000 , 1]])\n",
    "labels = np.array([1, 0, 0, 1, 1, 1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1 What is our expected accuracy for the baseline case where we predict one label for all rows? (*2 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BEGIN Workspace 1.1\n",
    "\n",
    "accuracy of the baseline using no features is equal to (number of time we predit that label divided by the total).\n",
    "For example if for college degree we predit yes for all rows, then 6/10 =.6 accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we have to find a way to deal with the continuous features. For now, let's put them into binary bins based on threshold arguments to our classifier - so we can treat this as a tuning parameter.\n",
    "\n",
    "1.2 Complete `threshold_features` to convert age and salary features to binary ones using the threshold arguments. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def threshold_features(features, age_threshold, salary_threshold):\n",
    "    binary_X = features * 1 #This row just creates a \"hard copy\" of the X array so we can manipulate it as needed\n",
    "    #BEGIN Workspace 1.2\n",
    "    #TODO: Threshold the corresponding features\n",
    "    for i in range(len(features)):\n",
    "        #case to check on age, 1 if bigger or equal to threshold and 0 if not\n",
    "        if(features[i][0] >=age_threshold):\n",
    "            binary_X[i][0] = 1\n",
    "        else:\n",
    "            binary_X[i][0] = 0\n",
    "        #case to check on salary, 1 if bigger or equal to threshold and 0 if not\n",
    "        if(features[i][1] >= salary_threshold):\n",
    "            binary_X[i][1] = 1\n",
    "        else:\n",
    "            binary_X[i][1] = 0\n",
    "            \n",
    "    #END Workspace 1.2\n",
    "\n",
    "    return binary_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "new_features = threshold_features(features, 30, 50000)\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As seen during the class, given a row $(x_1, x_2, x_3)$, the naive Bayes classifier should assign the label $y$ that\n",
    "maximizes:\n",
    "\n",
    "\\begin{align}\n",
    "\\log [p(y) \\prod_i p(x_i | y)] = \\log p(y) + \\sum_{i} \\log p(x_i | y)\n",
    "\\end{align}\n",
    "\n",
    "$p(y)$ and $p(x_i | y)$ are computed using the training set (during `fit` call).\n",
    "\n",
    "We have defined $p(x_i | y)$ as :\n",
    "\\begin{align}\n",
    "p(x_i | y) = \\frac{N_{y,i}}{N_y}\n",
    "\\end{align}\n",
    "where $N_{y,i}$ is the number of rows where $y$ and $x_i$ occur together, and $N_y = \\sum_i N_{y,i}$.\n",
    "\n",
    "1.3 Complete the `fit` call by computing the counts and joint counts. Hint: Use `features_counts` to store the contingency\n",
    "table $N_{y,i}$ for each feature $i$ and then use them to compute $\\log p(x_i | y)$ (*10 points*)\n",
    "\n",
    "1.4 Complete the `predict` call (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \"\"\"\n",
    "    NaiveBayes classifier for binary features and binary labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_counts = {}\n",
    "        self.features_counts = {}\n",
    "        self.classes_log_probabilities = {}\n",
    "        self.features_log_probabilities = {} #note i changed this from [] to {}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: binary np.array of shape (n_samples, n_features)\n",
    "        y: corresponding labels of shape (n_samples,)\n",
    "        Returns\n",
    "        -------\n",
    "        Trained classifier\n",
    "        \"\"\"\n",
    "\n",
    "        #BEGIN Workspace 1.3\n",
    "\n",
    "        #TODO: Compute the counts and joint counts\n",
    "        #First lets count the classes.\n",
    "        unique_classes = np.unique(y)\n",
    "        #initilize step\n",
    "        for label in unique_classes:\n",
    "            self.classes_counts[label]=0\n",
    "        #count step\n",
    "        for label in y:\n",
    "            self.classes_counts[label] =self.classes_counts[label] +1\n",
    "            \n",
    "        #now time to count p(xi|y)\n",
    "        #we know our data can only be either 0 or 1 (young or old, poor or rich...)\n",
    "        for i in range(1,len(X[0])+1):\n",
    "            for label in unique_classes:\n",
    "                #if i is zero then make it negative, if i is 1 then make it positive\n",
    "                self.features_counts[(-i,label)] =0\n",
    "                self.features_counts[(i,label)] =0\n",
    "\n",
    "        #lets count for self.features_counts\n",
    "        for row_index in range(len(X)):\n",
    "            for i in range(1,len(X[row_index])+1):\n",
    "                element_index = i-1\n",
    "                if X[row_index][element_index] == 0:\n",
    "                    self.features_counts[(-i,y[row_index])] = self.features_counts[(-i,y[row_index])] +1\n",
    "                elif X[row_index][element_index] == 1:\n",
    "                    self.features_counts[(i,y[row_index])] = self.features_counts[(i,y[row_index])] +1\n",
    "\n",
    "        #get features_counts totals for each class (0,1)\n",
    "        features_counts = self.features_counts\n",
    "        total_0 = 0\n",
    "        total_1 = 0\n",
    "        for x in features_counts:\n",
    "            if x[1] == 0:\n",
    "                total_0 = total_0 +features_counts[x] \n",
    "            if x[1] == 1:\n",
    "                total_1 = total_1 +features_counts[x] \n",
    "        # compute log probabilities\n",
    "        for x in features_counts:\n",
    "            if x[1] == 0:\n",
    "                self.features_log_probabilities[x] = np.log((features_counts[x]/total_0))\n",
    "            elif x[1] == 1:\n",
    "                self.features_log_probabilities[x] = np.log(features_counts[x]/total_1)\n",
    "\n",
    "        #compute log probability of each class\n",
    "        classes_counts = self.classes_counts\n",
    "        for x in classes_counts:\n",
    "            self.classes_log_probabilities[x] = math.log(classes_counts[x]/sum(classes_counts.values()))\n",
    "            \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        # sum the feature log probabilities for each class\n",
    "        log_total= [0,0]\n",
    "\n",
    "        features_log_probabilities = self.features_log_probabilities\n",
    "        for x in features_log_probabilities:\n",
    "            if x[1] == 0:\n",
    "                log_total[0] = log_total[0] + features_log_probabilities[x]\n",
    "            elif x[1] == 1:\n",
    "                log_total[1] = log_total[1] + features_log_probabilities[x]\n",
    "        \n",
    "        joint_log_likelihood = np.zeros((3, 2))\n",
    "        #joint_log_likelihood = np.zeros((x_test.shape[0], self.classes_counts.shape[0]))\n",
    "        y_hat = 0\n",
    "        #BEGIN Workspace 1.4\n",
    "        #TODO: Find the corresponding labels using Naive bayes logic\n",
    "        final_predict =[]\n",
    "        for x in range(len(x_test)):\n",
    "            predict_row = []\n",
    "            for label in self.classes_log_probabilities:\n",
    "                total_right =0\n",
    "                for col in range(np.shape(x_test)[1]):\n",
    "                    if x_test[x][col] == 0:\n",
    "                        key =  -(col+1)\n",
    "                    elif x_test[x][col] == 1:\n",
    "                        key =  col+1\n",
    "                    for y in features_log_probabilities:\n",
    "                        if y[1] == label:\n",
    "                            if y[0] == key:\n",
    "                                total_right = total_right + features_log_probabilities[y]\n",
    "                \n",
    "                predict_row.append(self.classes_log_probabilities[label]+total_right)\n",
    "            final_predict.append(predict_row.index(max(predict_row)))\n",
    "\n",
    "        return final_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.5 Using age 30 and salary 40,000 as thresholds, transform the features and evaluate (accuracy) the NaiveBayes classifier\n",
    "on the training data. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given labels: [1 0 0 1 1 1 1 0 0 1]\n",
      "prediction result: [1, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "#BEGIN Workspace 1.5\n",
    "#TODO: Transform features to binary features, fit the classifier, report the accuracy\n",
    "new_features = threshold_features(features, 30, 40000)\n",
    "model = NaiveBayes()\n",
    "model =model.fit(new_features,labels)\n",
    "result =model.predict(new_features)\n",
    "print('given labels:',labels)\n",
    "print(\"prediction result:\", result)\n",
    "correct_predict = 0\n",
    "for i in range(len(result)):\n",
    "    if result[i] == labels[i]:\n",
    "        correct_predict = correct_predict + 1\n",
    "print('Accuracy:', correct_predict/len(result))\n",
    "#END Workspace 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bonus question** 1.6 Use the attribute `alpha` of the NaiveBayes to convert it to the smoothed NaiveBayes presented during the class. (*5 points*)\n",
    "\n",
    "Solution: I copy my solution above and modified it.specifically changed lines 69,71 and 76."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes_smoothed(object):\n",
    "    \"\"\"\n",
    "    NaiveBayes classifier for binary features and binary labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_counts = {}\n",
    "        self.features_counts = {}\n",
    "        self.classes_log_probabilities = {}\n",
    "        self.features_log_probabilities = {} #note i changed this from [] to {}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: binary np.array of shape (n_samples, n_features)\n",
    "        y: corresponding labels of shape (n_samples,)\n",
    "        Returns\n",
    "        -------\n",
    "        Trained classifier\n",
    "        \"\"\"\n",
    "\n",
    "        #BEGIN Workspace 1.3\n",
    "\n",
    "        #TODO: Compute the counts and joint counts\n",
    "        #First lets count the classes.\n",
    "        unique_classes = np.unique(y)\n",
    "        #initilize step\n",
    "        for label in unique_classes:\n",
    "            self.classes_counts[label]=0\n",
    "        #count step\n",
    "        for label in y:\n",
    "            self.classes_counts[label] =self.classes_counts[label] +1\n",
    "            \n",
    "        #now time to count p(xi|y)\n",
    "        #we know our data can only be either 0 or 1 (young or old, poor or rich...)\n",
    "        for i in range(1,len(X[0])+1):\n",
    "            for label in unique_classes:\n",
    "                #if i is zero then make it negative, if i is 1 then make it positive\n",
    "                self.features_counts[(-i,label)] =0\n",
    "                self.features_counts[(i,label)] =0\n",
    "\n",
    "        #lets count for self.features_counts\n",
    "        for row_index in range(len(X)):\n",
    "            for i in range(1,len(X[row_index])+1):\n",
    "                element_index = i-1\n",
    "                if X[row_index][element_index] == 0:\n",
    "                    self.features_counts[(-i,y[row_index])] = self.features_counts[(-i,y[row_index])] +1\n",
    "                elif X[row_index][element_index] == 1:\n",
    "                    self.features_counts[(i,y[row_index])] = self.features_counts[(i,y[row_index])] +1\n",
    "\n",
    "        #get features_counts totals for each class (0,1)\n",
    "        features_counts = self.features_counts\n",
    "        total_0 = 0\n",
    "        total_1 = 0\n",
    "        for x in features_counts:\n",
    "            if x[1] == 0:\n",
    "                total_0 = total_0 +features_counts[x] \n",
    "            if x[1] == 1:\n",
    "                total_1 = total_1 +features_counts[x] \n",
    "                \n",
    "        # compute log probabilities\n",
    "        for x in features_counts:\n",
    "            if x[1] == 0:\n",
    "                self.features_log_probabilities[x] = np.log(((features_counts[x]+1)/(total_0+len(unique_classes))))\n",
    "            elif x[1] == 1:\n",
    "                self.features_log_probabilities[x] = np.log((features_counts[x]+1)/(total_1+len(unique_classes)))\n",
    "\n",
    "        #compute log probability of each class\n",
    "        classes_counts = self.classes_counts\n",
    "        for x in classes_counts:\n",
    "            self.classes_log_probabilities[x] = math.log(classes_counts[x]+1/sum(classes_counts.values())+len(unique_classes))\n",
    "            \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        # sum the feature log probabilities for each class\n",
    "        log_total= [0,0]\n",
    "\n",
    "        features_log_probabilities = self.features_log_probabilities\n",
    "        for x in features_log_probabilities:\n",
    "            if x[1] == 0:\n",
    "                log_total[0] = log_total[0] + features_log_probabilities[x]\n",
    "            elif x[1] == 1:\n",
    "                log_total[1] = log_total[1] + features_log_probabilities[x]\n",
    "        \n",
    "        joint_log_likelihood = np.zeros((3, 2))\n",
    "        #joint_log_likelihood = np.zeros((x_test.shape[0], self.classes_counts.shape[0]))\n",
    "        y_hat = 0\n",
    "        #BEGIN Workspace 1.4\n",
    "        #TODO: Find the corresponding labels using Naive bayes logic\n",
    "        final_predict =[]\n",
    "        for x in range(len(x_test)):\n",
    "            predict_row = []\n",
    "            for label in self.classes_log_probabilities:\n",
    "                total_right =0\n",
    "                for col in range(np.shape(x_test)[1]):\n",
    "                    if x_test[x][col] == 0:\n",
    "                        key =  -(col+1)\n",
    "                    elif x_test[x][col] == 1:\n",
    "                        key =  col+1\n",
    "                    for y in features_log_probabilities:\n",
    "                        if y[1] == label:\n",
    "                            if y[0] == key:\n",
    "                                total_right = total_right + features_log_probabilities[y]\n",
    "                \n",
    "                predict_row.append(self.classes_log_probabilities[label]+total_right)\n",
    "            final_predict.append(predict_row.index(max(predict_row)))\n",
    "\n",
    "        return final_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given labels: [1 0 0 1 1 1 1 0 0 1]\n",
      "prediction result: [1, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "#smoothed version\n",
    "new_features = threshold_features(features, 30, 40000)\n",
    "model = NaiveBayes_smoothed()\n",
    "model =model.fit(new_features,labels)\n",
    "result =model.predict(new_features)\n",
    "print('given labels:',labels)\n",
    "print(\"prediction result:\", result)\n",
    "correct_predict = 0\n",
    "for i in range(len(result)):\n",
    "    if result[i] == labels[i]:\n",
    "        correct_predict = correct_predict + 1\n",
    "print('Accuracy:', correct_predict/len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 2 - Decision trees [25 points]\n",
    "***\n",
    "The goal of this problem is to implement the core elements of the Decision Tree classifier.\n",
    "We do not expect a highly efficient implementation of the functions since the ensemble methods will use the implementation\n",
    "from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering the variable *Colorado residency*.\n",
    "\n",
    "The leaf nodes of a decision tree act in the same way as in question (1.1) where no feature is used.\n",
    "\n",
    "2.1 Complete `get_error_in_leaf` to return the count of misclassified instances. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_error_in_leaf(y, indices):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param indices: the subset of indexes in the leaf node\n",
    "    :return: Returns the number of errors in a leaf node of a decision tree.\n",
    "    \"\"\"\n",
    "    classes_counts = {}\n",
    "    unique_classes = np.unique(y)\n",
    "    #initilize step\n",
    "    for label in unique_classes:\n",
    "        classes_counts[label]=0\n",
    "        \n",
    "    for i in indices:\n",
    "        \n",
    "        classes_counts[y[i]] = classes_counts[y[i]] +1\n",
    "        \n",
    "\n",
    "    error_count = classes_counts[0]\n",
    "    #BEGIN Workspace 2.1\n",
    "    #TODO: Compute the number of errors in the leaf node (no feature is used)\n",
    "    \n",
    "    #END Workspace 2.1\n",
    "    return error_count\n",
    "\n",
    "\n",
    "def value_split_binary_feature(x, y, feature_index, root, criteria_func):\n",
    "    \"\"\"Will be used later to evaluate the criteria gain\"\"\"\n",
    "    left_child = [i for i in root if x[i, feature_index] == 0]\n",
    "    right_child = [i for i in root if x[i, feature_index] == 1]\n",
    "    return criteria_func(y, root, left_child, right_child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([1, 0, 0, 1, 1, 1, 1, 0, 0, 1])\n",
    "get_error_in_leaf(labels, [2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use information gain criteria to decide how to split the root node of our decision tree.\n",
    "\n",
    "2.2 Complete the `entropy` function. (*5 points*)\n",
    "\n",
    "2.3 Complete the `information_gain_criteria` to compute the information gained by splitting the root node.\n",
    " Print the gain value for splitting based on *Colorado residency* (*5 points*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y, indices):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param indices: the indices of data points\n",
    "    :return: Returns the entropy in the labels for the data points in indices.\n",
    "    \"\"\"\n",
    "    percent_lst = []\n",
    "    classes_counts = {}\n",
    "    unique_classes = np.unique(y)\n",
    "    #initilize step\n",
    "    for label in unique_classes:\n",
    "        classes_counts[label]=0\n",
    "        \n",
    "    entropy_value = 0\n",
    "    if len(indices) == 0: # deal with corner case when there is no data point.\n",
    "        return entropy_value\n",
    "    else:\n",
    "        #BEGIN Workspace 2.2\n",
    "        #TODO: Compute the entropy of the labels from indices\n",
    "        for i in indices:\n",
    "            classes_counts[y[i]] = classes_counts[y[i]] + 1 \n",
    "        for i in classes_counts:\n",
    "            percent_lst.append((classes_counts[i]/sum(classes_counts.values())))\n",
    "\n",
    "        #now calculate entrpoy\n",
    "        for i in percent_lst:\n",
    "            entropy_value = entropy_value - (i * np.log2(i))\n",
    "        #END Workspace 2.2\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param root: indices of all the data points in the root\n",
    "    :param left_child: the subset of indices in the left child\n",
    "    :param right_child: the subset of indices in the right child\n",
    "    :return: information gain of the split\n",
    "    \"\"\"\n",
    "    information_gain = 0\n",
    "    #BEGIN Workspace 2.3.a\n",
    "    #TODO: Compute the information gain of the split\n",
    "    i_dpar =entropy(y,root)\n",
    "    i_dright= entropy(y,right_child)\n",
    "    i_dleft=entropy(y,left_child)\n",
    "    dpar_size = len(root)\n",
    "    dright_size = len(right_child)\n",
    "    dleft_size = len(left_child)\n",
    "    information_gain = (i_dpar - ((dleft_size/dpar_size) * i_dleft) - ((dright_size/dpar_size) * i_dright))\n",
    "    #END Workspace 2.3.a\n",
    "\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy test: 0.9182958340544896\n",
      "Information gain test: 0.0912774462416801\n"
     ]
    }
   ],
   "source": [
    "feature_id = 2\n",
    "info_gain = 0 \n",
    "#BEGIN Workspace 2.3.b\n",
    "#TODO: report the information gain of the split based on Colorado Residency\n",
    "\n",
    "print('Entropy test:',entropy(labels, [2,3,4]))\n",
    "root = []\n",
    "#make the root node as tuples example: (colorado_residency,has_degree)\n",
    "for i in range(len(new_features)):\n",
    "    root.append((new_features[i][feature_id],labels[i]))\n",
    "#branch off of colorado residency status\n",
    "left_child,right_child =[],[]\n",
    "for i in range(len(root)):\n",
    "    if root[i][0] == 0:\n",
    "        left_child.append(i)\n",
    "    elif root[i][0] == 1:\n",
    "        right_child.append(i)\n",
    "print('Information gain test:',information_gain_criteria(labels, [0,1,2,3,4,5,6,7,8,9], left_child, right_child))\n",
    "      \n",
    "#END Workspace 2.3.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to deal with continuous features for the decision tree.\n",
    "One way to deal with continuous (or ordinal) data is to define binary features based on thresholding as we've done\n",
    "for NaiveBayes. But we have to find the optimal threshold based on the criteria we're using.\n",
    "\n",
    "2.4 Complete the `value_split_continuous_feature` by trying different possible threshold values of feature\n",
    "of index `feature_index` and return the best criteria value and threshold. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def value_split_continuous_feature(x, y, feature_index, root, criteria_func=information_gain_criteria):\n",
    "    \"\"\"\n",
    "    :param x: all feature values\n",
    "    :param y: all labels\n",
    "    :param feature_index: feature id to split the tree based on\n",
    "    :param root: indexes of all the data points in the root\n",
    "    :param criteria_func: the splitting criteria function\n",
    "    :return: Return the best value and its corresponding threshold by splitting based on a continuous feature.\n",
    "    \"\"\"\n",
    "    all_values = {}\n",
    "\n",
    "    best_value, best_thres = 0, 0\n",
    "    feature_values = []\n",
    "    for row in x:\n",
    "        feature_values.append(row[feature_index])\n",
    "    index = 0\n",
    "    for split in feature_values:\n",
    "        counter = 0\n",
    "        left_child,right_child = [],[]\n",
    "        for value in feature_values:\n",
    "            if value > split:\n",
    "                right_child.append(counter)\n",
    "            elif value <= split:\n",
    "                left_child.append(counter)\n",
    "            counter= counter +1\n",
    "        all_values[index]=criteria_func(y,root,left_child,right_child)\n",
    "        index = index +1\n",
    "    best_value = max(all_values.values())\n",
    "    for key in all_values:\n",
    "        if all_values[key] == best_value:\n",
    "            best_thres = feature_values[key]\n",
    "\n",
    "\n",
    "\n",
    "    return best_value, best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log2\n",
      "C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.12451124978365313, 32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = [x for x in range(len(features))]\n",
    "value_split_continuous_feature(features, labels, 0, root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.5 Find the best thresholds for age and salary. Print their corresponding information gains. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age best split: (0.12451124978365313, 32)\n",
      "Age best split: (0.0912774462416801, 0)\n",
      "Salary best split: (0.12451124978365313, 45000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log2\n",
      "C:\\Users\\Tamer\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "root = list(range(len(labels))) # root includes all data points\n",
    "\n",
    "#BEGIN Workspace 2.5\n",
    "#TODO: Report the best thresholds for age and salary and their split information gains\n",
    "age_index = 0\n",
    "salary_index = 1\n",
    "print('Age best split:',value_split_continuous_feature(features, labels, age_index, root))\n",
    "print('Age best split:',value_split_continuous_feature(features, labels, 2, root))\n",
    "\n",
    "print('Salary best split:',value_split_continuous_feature(features, labels, salary_index, root))\n",
    "#END Workspace 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Based on the obtained information gains, if we build a decision stump (decision tree with depth 1) greedily,\n",
    "which feature should we choose? Why? What's the resulting accuracy? (*2 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#BEGIN Workspace 2.6.a\n",
    "\n",
    "Based on the information gains we choose either split age or salary because they have higher information gain(they are equal). For the salary split we get higher accuracy so it would be better to split on it\n",
    "\n",
    "#END Workspace 2.6.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "#BEGIN Workspace 2.6.b\n",
    "\n",
    "labels = np.array([1, 0, 0, 1, 1, 1, 1, 0, 0, 1])\n",
    "new_features = threshold_features(features, 32, 45000)\n",
    "counter  =0\n",
    "for x in range(len(new_features)): \n",
    "    val = new_features[x][2]\n",
    "    #print(val)\n",
    "    if val == labels[x]:\n",
    "        counter = counter +1\n",
    "print(counter/len(labels))\n",
    "#get_error_in_leaf(labels, [2,3,4])\n",
    "#BEGIN Workspace 2.6.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**\n",
    "\n",
    "2.7 You now have all the ingredients to build a decision tree recursively.\n",
    "You can build a decision tree of depth two and report its classification error on the training data and the tree.(*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#BEGIN Workspace 2.7\n",
    "#TODO: Build a Decision Tree of Depth 2 using age, salary and the previously computed thresholds\n",
    "\n",
    "\n",
    "#END Workspace 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3  - Decision Tree Ensembles: Bagging and (BONUS: Boosting) [50 points]\n",
    "---\n",
    "\n",
    "We are going to predict house price levels using decision tree ensembles.\n",
    "\n",
    "In this classification problem, we compare Decision trees and it's ensembles - Bagging and Boosting on House Price prediction [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "Our *weak learner* for this problem is the DecisionTreeClassifier from scikit-learn with `max_depth=10`.\n",
    "\n",
    "We start first by loading preprocessed data that we'll use. Since the original data is for regression, we have first to transform\n",
    "`y_train` and `y_test` to discrete values reflecting price level.\n",
    "\n",
    "|Price range| Label|\n",
    "|:----------:|--:|\n",
    "| $ P < $125000|0|\n",
    "|125000$\\leq P < $ 160000| 1 |\n",
    "|160000$ \\leq P < $ 200000| 2 |\n",
    "|200000$ \\leq P $ | 3 |\n",
    "\n",
    "3.1 Start by transforming`y_train` and `y_test` to discrete values using the provided ranges. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] (1166, 79)\n",
      "[0 1 2 3] (292, 79)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/test_train.pkl','rb'))\n",
    "#BEGIN Workspace 3.1\n",
    "#TODO: Discretize y_train and y_test\n",
    "for index in range(len(y_train)):\n",
    "        if y_train[index] < 125000:\n",
    "            y_train[index] = 0\n",
    "        elif y_train[index] >= 125000 and y_train[index] < 160000:\n",
    "            y_train[index] = 1\n",
    "        elif y_train[index] >= 160000 and y_train[index]< 200000:\n",
    "            y_train[index] = 2\n",
    "        elif y_train[index] >= 200000:\n",
    "            y_train[index]= 3\n",
    "for index in range(len(y_test)):\n",
    "\n",
    "        if y_test[index] < 125000:\n",
    "            y_test[index] = 0\n",
    "        elif y_test[index] >= 125000 and y_test[index] < 160000:\n",
    "            y_test[index] = 1\n",
    "        elif y_test[index] >= 160000 and y_test[index]< 200000:\n",
    "            y_test[index] = 2\n",
    "        elif y_test[index] >= 200000:\n",
    "            y_test[index]= 3\n",
    "\n",
    "#END Workspace 3.1\n",
    "print(np.unique(y_train), X_train.shape)\n",
    "print(np.unique(y_test), X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.2 Complete the `ensemble_test` class to `fit` the model received as parameter and store the metrics and running time. (*5 points*)\n",
    "\n",
    "3.3 Complete `plot_metric` to show and compare different statistics of each model in a bar chart. (*5 points*)\n",
    "\n",
    "Later we will also use `ensemble_test` class to plot score, metric and time taken to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_weak_learner():\n",
    "    \"\"\"Return a new instance of out chosen weak learner\"\"\"\n",
    "    return DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "class EnsembleTest:\n",
    "    \"\"\"\n",
    "        Test multiple model performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        initialize data partitions\n",
    "        \"\"\"\n",
    "        self.scores = {}\n",
    "        self.execution_time = {}\n",
    "        self.metric = {}\n",
    "        self.x_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.score_name ='Mean accuracy'\n",
    "        self.metric_name = 'Precision(micro)'\n",
    "\n",
    "    def fit_model(self, model, name):\n",
    "        \"\"\"\n",
    "        Fit the model on train data.\n",
    "        predict on test and store score and execution time for each fit.\n",
    "        :param model: model\n",
    "        :param name: name of model\n",
    "        \"\"\"\n",
    "        start = time()\n",
    "        #BEGIN Workspace 3.2\n",
    "        #TODO: Fit the model and get the predictions\n",
    "        #train and test data are treated as global variables\n",
    "        \n",
    "        #model = DecisionTreeClassifier(model)\n",
    "        model =model.fit(self.x_train,self.y_train)\n",
    "        prediction= model.predict(self.x_test)\n",
    "        #get precision\n",
    "        self.scores[name] = precision_score(prediction,self.y_test,average=\"micro\")\n",
    "        #get accuracy\n",
    "        counter = 0\n",
    "        for x in range(len(prediction)):\n",
    "            if prediction[x] == self.y_test[x]:\n",
    "                counter = counter +1\n",
    "        self.metric[name]=counter/len(self.y_test)\n",
    "\n",
    "        #END Workspace 3.2\n",
    "        self.execution_time[name] = time() - start\n",
    "\n",
    "    def print_result(self):\n",
    "        \"\"\"\n",
    "            print results for all models trained and tested.\n",
    "        \"\"\"\n",
    "        models_cross = pd.DataFrame({\n",
    "            'Model'         : list(self.metric.keys()),\n",
    "             self.score_name     : list(self.scores.values()),\n",
    "             self.metric_name    : list(self.metric.values()),\n",
    "            'Execution time': list(self.execution_time.values())})\n",
    "        print(models_cross.sort_values(by=self.score_name, ascending=False))\n",
    "\n",
    "    def plot_metric(self):\n",
    "        #BEGIN Workspace 3.3\n",
    "        #TODO: plot bar chart for each metric : time, metric, score\n",
    "        names = []\n",
    "        accuracy = []\n",
    "        precision =[]\n",
    "        time = []\n",
    "        for name in self.scores:\n",
    "            names.append(name)\n",
    "            accuracy.append(self.scores[name])\n",
    "        for name in self.metric:\n",
    "            precision.append(self.metric[name])\n",
    "        for name in self.execution_time:\n",
    "            time.append(self.execution_time[name])\n",
    "        # Initialize figure and axis\n",
    "        #fig, ax = plt.subplots(figsize=(8,4))\n",
    "        N = len(names)\n",
    "\n",
    "        ind = np.arange(N) \n",
    "        width = 0.30     \n",
    "        plt.bar(ind, accuracy, width, label='Accuracy')\n",
    "        plt.bar(ind + width, precision, width,label='Precision')\n",
    "        plt.bar(ind + width+width, time, width,label='time')\n",
    "        plt.ylabel('Results')\n",
    "        plt.title('Results By Model')\n",
    "\n",
    "        plt.xticks(ind + (width*2) / 2,names)\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        #END Workspace 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.4 Test `EnsembleTest` using our weak learner returned by `get_weak_learner` (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model  Mean accuracy  Precision(micro)  Execution time\n",
      "0  model1       0.678082          0.678082        0.027089\n",
      "1  model2       0.671233          0.671233        0.020946\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(X_train,y_train,X_test,y_test)\n",
    "#BEGIN Workspace 3.4\n",
    "#TODO: Initialize weak learner and fit ensemble_handler\n",
    "\n",
    "model = get_weak_learner()\n",
    "model2 = get_weak_learner()\n",
    "ensemble_handler.fit_model(model,'model1')\n",
    "ensemble_handler.fit_model(model2,'model2')\n",
    "\n",
    "\n",
    "ensemble_handler.print_result()\n",
    "ensemble_handler.plot_metric()\n",
    "#END Workspace 3.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging:**\n",
    "\n",
    "Bagging consists of training a set of weak learners using random subsets of the train data.\n",
    "\n",
    "3.5 First, complete `sample_data` to return a random sample of size `sample_ratio * len(X_train)` of features and labels (*2 points*)\n",
    "\n",
    "3.6 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data (*5 points*)\n",
    "\n",
    "3.7 Complete `predict` method to return the most likely label by combining different estimators predictions. \n",
    "Use a simple majority / plurality vote system similar to the one used in your KNNClassifier in Problem Set 1. However, in this case, to break a tie you should use `predict_log_proba` or `predict_proba` method of DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba) (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaggingEnsemble(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.estimators = []\n",
    "\n",
    "    def sample_data(self, x_train, y_train):\n",
    "        x_sample, y_sample = [], []\n",
    "        #BEGIN Workspace 3.5\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train)\n",
    "        total = len(x_train)\n",
    "        size = round(self.sample_ratio * total)\n",
    "        index_lst = [index for index in range(total)]\n",
    "        for i in range(size):        \n",
    "            #randomly select an index\n",
    "            choice = np.random.choice(index_lst,)\n",
    "            #Once we randomly select an index, append its element to the proper list.\n",
    "            x_sample.append(x_train[choice])\n",
    "            y_sample.append(y_train[choice])\n",
    "            # Manually remove the selected index so its not picked again\n",
    "            #index_lst.remove(choice)\n",
    "\n",
    "        #END Workspace 3.5\n",
    "        return x_sample, y_sample\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for i in range(self.n_estimators):\n",
    "            #BEGIN Workspace 3.6\n",
    "            #TODO: sample data and create new weak learned trained on the sample\n",
    "            x_sample,y_sample = self.sample_data(x_train,y_train)\n",
    "            model = get_weak_learner()\n",
    "            #model_name = 'model'+i\n",
    "            model =model.fit(x_sample,y_sample)\n",
    "            self.estimators.append(model)\n",
    "            #ensemble_handler.fit_model(model,'model')\n",
    "            #END Workspace 3.6\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = 0\n",
    "        answer = []\n",
    "        #BEGIN Workspace 3.7\n",
    "        prediction_lst = []\n",
    "        #TODO: go through the trained estimators and acummualte their predicted_proba to get the mostly likely label\n",
    "        for model in self.estimators:\n",
    "            prediction_lst.append(model.predict(X_test))\n",
    "            \n",
    "            \n",
    "        for i in range(len(X_test)):\n",
    "            holder_lst = []\n",
    "            holder_count = {}\n",
    "\n",
    "            for model in prediction_lst:\n",
    "                            \n",
    "                holder_lst.append(model[i])\n",
    "            unique = np.unique(holder_lst)\n",
    "            for i in unique:\n",
    "                holder_count[i] = 0\n",
    "            for i in holder_lst:\n",
    "                holder_count[i] = holder_count[i] + 1\n",
    "                \n",
    "            answer.append(max(holder_count.items(), key=operator.itemgetter(1))[0])\n",
    "        #END Workspace 3.7\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 1, 3, 1, 3, 3, 3, 0, 2, 2, 2, 3, 2, 2, 2, 1, 0, 2, 2, 2, 2, 3, 2, 2, 2, 0, 2, 1, 0, 0, 2, 0, 3, 3, 1, 2, 0, 0, 0, 3, 1, 0, 1, 2, 0, 1, 1, 0, 1, 0, 0, 1, 1, 3, 2, 1, 1, 0, 0, 2, 3, 3, 3, 1, 0, 0, 0, 2, 3, 1, 0, 1, 2, 2, 3, 2, 1, 0, 0, 0, 0, 0, 2, 3, 3, 2, 1, 1, 2, 0, 1, 3, 1, 2, 3, 3, 3, 3, 2, 3, 3, 2, 1, 2, 2, 3, 0, 3, 2, 0, 2, 1, 3, 0, 1, 0, 0, 3, 2, 1, 3, 2, 2, 0, 3, 2, 2, 1, 3, 0, 0, 2, 0, 2, 3, 0, 3, 3, 3, 3, 2, 0, 1, 0, 3, 2, 3, 3, 1, 1, 2, 1, 2, 1, 0, 1, 3, 2, 2, 0, 3, 1, 2, 2, 3, 3, 1, 3, 3, 0, 1, 3, 3, 1, 1, 1, 1, 0, 1, 1, 3, 1, 3, 2, 0, 2, 3, 2, 1, 3, 1, 1, 0, 3, 2, 0, 3, 0, 3, 2, 0, 3, 3, 2, 3, 1, 1, 0, 3, 2, 1, 2, 2, 3, 2, 1, 0, 1, 3, 3, 1, 2, 2, 1, 1, 0, 0, 0, 0, 2, 3, 1, 1, 1, 2, 1, 3, 2, 0, 3, 3, 3, 3, 2, 1, 2, 1, 0, 3, 1, 0, 0, 3, 2, 1, 3, 2, 0, 2, 1, 0, 3, 3, 3, 2, 1, 3, 2, 1, 3, 3, 0, 1, 1, 0, 3, 0, 2, 3, 0, 2, 3, 3, 1, 3, 0, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "models = BaggingEnsemble(10, 0.9)\n",
    "models.fit(X_train,y_train)\n",
    "print(models.predict(X_test))\n",
    "\n",
    "#ensemble_handler.fit_model(BaggingEnsemble(10, 0.9),'Bagging')\n",
    "#ensemble_handler.print_result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Random Forest**\n",
    "Random Forest has an additional layer of randomness compared to Bagging: we also sample a random subset of the features.\n",
    "\n",
    "3.8 First, complete `sample_data` to return a random sample of size `sample_ratio * len(X_train)` of labels and `feature_ratio * num_features` of features (*2 points*)\n",
    "\n",
    "3.9 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data.\n",
    "Make sure to keep track of the sampled features for each estimator to use them in the prediction step. (*5 points*)\n",
    "\n",
    "3.10 Complete `predict` method to return the most likely label by combining different estimators predictions.\n",
    "Use a simple majority / plurality vote system similar to the one used in your KNNClassifier in Problem Set 1. However, in this case, to break a tie you should use `predict_log_proba` or `predict_proba`  method of DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba) (*3 points*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio, features_ratio):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.features_ratio = features_ratio\n",
    "        self.estimators = []\n",
    "        self.features_indices = []\n",
    "\n",
    "    def sample_data(self, x_train, y_train):\n",
    "        X_sample, y_sample, features_indices = [],[],[]\n",
    "        #BEGIN Workspace 3.8\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train) and subset of features of size\n",
    "        #         features_ratio * num_features\n",
    "\n",
    "        total = len(x_train)\n",
    "        len_features = len(x_train[0])\n",
    "        size_sample = round(self.sample_ratio * total)\n",
    "        size_feature = round(self.features_ratio* len_features)\n",
    "        index_lst = [index for index in range(total)]\n",
    "        feature_index = [index for index in range(size_feature)]\n",
    "        \n",
    "        for i in range(size_feature):        \n",
    "            #randomly select an index\n",
    "            choice = np.random.choice(feature_index)\n",
    "            #Once we randomly select an index, append its element to the proper list.\n",
    "            features_indices.append(choice)\n",
    "            # Manually remove the selected index so its not picked again\n",
    "            feature_index.remove(choice)\n",
    "        \n",
    "        \n",
    "        for i in range(size_sample):        \n",
    "            #randomly select an index\n",
    "            choice = np.random.choice(index_lst)\n",
    "            #Once we randomly select an index, append its element to the proper list.\n",
    "            X_sample.append(x_train[choice])\n",
    "            y_sample.append(y_train[choice])\n",
    "            # Manually remove the selected index so its not picked again\n",
    "            #index_lst.remove(choice)\n",
    "\n",
    "\n",
    "        #END Workspace 3.8\n",
    "        return X_sample, y_sample, features_indices\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for _ in range(self.n_estimators):\n",
    "            #BEGIN Workspace 3.9\n",
    "            #TODO: sample data with random subset of rows and features using sample_data\n",
    "            #Hint: keep track of the features indices in features_indices to use in predict\n",
    "            x_sample,y_sample,feature_sample = self.sample_data(x_train,y_train)\n",
    "            model = get_weak_learner()\n",
    "            #need to create new list to delete features not included in featire_sample (indcies)\n",
    "            new_xsample,new_ysample = [],[]\n",
    "            for row in range(len(x_sample)):\n",
    "                new_row_x = []\n",
    "                for y in feature_sample:\n",
    "                    new_row_x.append(x_sample[row][y])\n",
    "                new_ysample.append(y_sample[row])\n",
    "                new_xsample.append(new_row_x)\n",
    "            \n",
    "            model =model.fit(x_sample,y_sample)\n",
    "            self.estimators.append(model)\n",
    "            #ensemble_handler.fit_model(model,'model')\n",
    "            #END Workspace 3.9\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = 0\n",
    "        answer = []\n",
    "        prediction_lst = []\n",
    "        #TODO: go through the trained estimators and acummualte their predicted_proba to get the mostly likely label\n",
    "        for model in self.estimators:\n",
    "            prediction_lst.append(model.predict(X_test))\n",
    "            \n",
    "            \n",
    "        for i in range(len(X_test)):\n",
    "            holder_lst = []\n",
    "            holder_count = {}\n",
    "\n",
    "            for model in prediction_lst:\n",
    "                            \n",
    "                holder_lst.append(model[i])\n",
    "            unique = np.unique(holder_lst)\n",
    "            for i in unique:\n",
    "                holder_count[i] = 0\n",
    "            for i in holder_lst:\n",
    "                holder_count[i] = holder_count[i] + 1\n",
    "                \n",
    "            answer.append(max(holder_count.items(), key=operator.itemgetter(1))[0])\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 0, 1, 3, 1, 3, 3, 3, 1, 2, 2, 2, 3, 2, 2, 2, 1, 1, 3, 2, 3, 2, 3, 2, 2, 2, 0, 2, 1, 0, 0, 2, 0, 3, 3, 1, 2, 0, 0, 0, 3, 0, 0, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 3, 2, 1, 1, 0, 0, 2, 3, 3, 3, 3, 0, 0, 0, 2, 3, 1, 0, 1, 2, 2, 3, 2, 1, 1, 0, 0, 0, 0, 1, 3, 3, 2, 1, 1, 2, 1, 1, 3, 0, 2, 3, 3, 3, 3, 2, 3, 3, 2, 1, 2, 2, 3, 0, 3, 2, 0, 2, 1, 3, 0, 2, 0, 0, 3, 2, 1, 3, 2, 2, 1, 3, 2, 2, 1, 3, 0, 1, 1, 0, 2, 3, 1, 3, 3, 3, 3, 2, 0, 1, 0, 3, 2, 3, 3, 0, 1, 2, 3, 3, 1, 0, 1, 3, 2, 2, 0, 3, 1, 2, 3, 3, 3, 0, 3, 3, 0, 1, 3, 3, 1, 1, 1, 1, 0, 1, 1, 3, 2, 3, 2, 0, 3, 3, 2, 1, 3, 1, 1, 0, 3, 2, 0, 3, 1, 3, 2, 0, 3, 3, 3, 3, 1, 1, 0, 3, 1, 0, 2, 2, 3, 3, 1, 1, 0, 3, 3, 2, 2, 2, 1, 1, 0, 0, 0, 0, 2, 2, 1, 1, 1, 2, 1, 3, 2, 0, 3, 3, 3, 3, 3, 1, 2, 1, 0, 3, 1, 1, 0, 3, 1, 1, 3, 2, 0, 2, 1, 1, 3, 3, 3, 2, 1, 3, 2, 1, 2, 3, 0, 1, 0, 0, 3, 0, 2, 3, 0, 2, 3, 3, 1, 3, 0, 3, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "# This cell should run without errors\n",
    "models = RandomForest(20, sample_ratio=0.9, features_ratio=0.8)\n",
    "#print(models.sample_data(X_train,y_train))\n",
    "models.fit(X_train,y_train)\n",
    "print(models.predict(X_test))\n",
    "#ensemble_handler.fit_model(RandomForest(20, sample_ratio=0.9, features_ratio=0.8), 'RandomForest')\n",
    "#ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting (BONUS 10 Points)**\n",
    "\n",
    "There are different methods of boosting, but we'll focus in this problem on Adaptive Boosting (AdaBoost).\n",
    "The logic of AdaBoost is to \"push\" the new learner to give more importance to previously misclassified data. We present\n",
    "below the multiclass variant of AdaBoost [SAMME](https://web.stanford.edu/~hastie/Papers/samme.pdf). We denote $K$ the number of classes.\n",
    "\n",
    "AdaBosst is performed by increasing the weights of misclassified simple after each iteration:\n",
    "- Start with equal weights $W_1 = (w_i), $ where   $w_i = \\frac{1}{\\texttt{n_samples}}$\n",
    "- at step j:\n",
    "    - Train estimator $h_j$ using weights $W_j$\n",
    "    - Find the weighted error rate $\\epsilon_j$ using $W_j$: $\\epsilon_j=\\frac{\\sum_i w_i \\delta(\\hat{y}_i, y_i)}{\\sum_i w_i}$\n",
    "    - Choose $\\alpha_j = \\log \\frac{1-\\epsilon_j}{\\epsilon_j} + \\log(K-1)$\n",
    "    - Update $W_j$ using: $w_i \\leftarrow w_i \\exp(\\alpha_j \\delta(\\hat{y_i}, y_i)) $\n",
    "    - Normalize $W_j$ to have sum 1\n",
    "- Global estimator is $H = \\sum_j \\alpha_j h_j$\n",
    "\n",
    "$\\hat{y}$ are the predicted labels, and $\\delta$ the Kronecker function, equal to 1 when the two argument are equal, 0 otherwise.\n",
    "\n",
    "\n",
    "Bonus.1 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on the same data but with different\n",
    "samples weights as detailed in the algorithm. Keep track of $(\\alpha_i)$\n",
    "\n",
    "Bonus.2 Complete `predict` method to return the predicted label using the global estimator $H$. Hint:\n",
    "use one hot encoding of the predicted labels from the weak learners and cumulate the prediction with weights $\\alpha_j$ \n",
    "\n",
    "Notice that if the weak learner is consistent (0 error rate on the training set), AdaBoost $\\alpha_j$ are no longer defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-26-9cac7400125d>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-9cac7400125d>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    def predict(self, X_test):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class AdaBoost(object):\n",
    "\n",
    "    def __init__(self, n_estimators, num_classes=4):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.num_classes = num_classes\n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "        self.weights = None\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        #BEGIN Workspace 3.11\n",
    "        #TODO: Implement Multiclass Adaboost and keep track of the alpha_j\n",
    "\n",
    "        #END Workspace 3.11\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        answer = 0\n",
    "        #BEGIN Workspace 3.12\n",
    "        #TODO: get the labels returned by the global estimator defined as H\n",
    "        #Hint: Use one-hot format to get the labels using the global estimator\n",
    "        #Hint: We don't need predict_proba for this one\n",
    "        \n",
    "        #END Workspace 3.12\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.fit_model(AdaBoost(10), 'AdaBoost')\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison**\n",
    "\n",
    "Bonus.3 Add different ensemble methods to the handler (try different parameters), plot, show, and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(X_train, y_train, X_test, y_test)\n",
    "decision = get_weak_learner()\n",
    "ensemble_handler.fit_model(decision,'decision_tree')\n",
    "#BEGIN Workspace 3.13.a\n",
    "#TODO Add multiple instances of the ensemble methods. Plot and compare their performance\n",
    "\n",
    "#END Workspace 3.13.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#BEGIN Workspace 3.13.b\n",
    "#TODO Comparison write-up\n",
    "#END Workspace 3.13.b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
